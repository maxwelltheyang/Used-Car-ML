# Used-Car-ML

The goal of this project was to predict car prices using a dataset called used_cars. It contained data on several thousand cars, each represented by key variables describing its make, model, condition, and other features. The dataset went through a lot of preprocessing and cleaning, from handling missing values, consolidating categorical levels, scaring various numerical variables, and removing outliers. There were also key transformations including converting long color descriptions into simpler colors and splitting columns into multiple columns to ensure that the data was well prepared.\
  Clustering techniques, including Hierarchical Clustering, K-means, and Spectral Clustering, were applied to explore the dataset and identify patterns among the cars. These methods highlighted the features that most influence similarity and provided insights to inform predictive modeling. To predict car prices, five regression models were implemented: Lasso Regression, Support Vector Machine (SVM) with an RBF kernel, Random Forest, Gradient Boosting, and K-Nearest Neighbors (KNN). Each model was tuned through cross-validation, with RMSE serving as the primary evaluation metric. Random Forest highlighted car models and mileage as key predictors but struggled with overfitting. Lasso Regression demonstrated strong generalizability, identifying engine size, manual transmission, and green exterior color as influential predictors. SVM and Gradient Boosting provided nuanced insights, with mileage, model year, and accident history consistently impacting price. KNN regression, though simple, offered competitive performance but explained less variance.\
  In our analysis, we incorporated unique methods beyond those reviewed in the literature, particularly in data preprocessing and model tuning. While the literature emphasized traditional supervised learning techniques like Random Forest and Lasso regression, we extended these approaches by introducing additional dimensionality reduction steps, such as consolidating categorical levels and removing rare categories to address overfitting risks in high-dimensional data. Furthermore, for Random Forest, we used 5-fold cross-validation for hyperparameter tuning instead of a fixed split approach, ensuring more robust parameter selection. For gradient boosting, we employed early stopping based on validation performance, a technique not mentioned in the reviewed studies, which helped mitigate overfitting while maintaining model complexity. AI tools were employed for code optimization, grammar correction, and model evaluation, adhering to academic integrity guidelines. This multi-method approach highlights the strengths and tradeoffs of predictive models in real-world applications.\
